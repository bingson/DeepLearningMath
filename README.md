# Deep Learning Mechanics

## 1_L-layer Neural Network

In this assignment you will implement all the building blocks of a 
neural network and use these building blocks to build a neural network 
of any architecture.

## 2_Initialization_Methods

Different initializations lead to different results. 
By completing this assignment you will learn how to use different initializations to
* speed up the convergence of gradient descent;
* increase the odds of gradient descent converging to a lower training (and generalization) error;
* break symmetry and make sure different hidden units can learn different things.

## 2_Optimization_Methods	

There are many different optimization algorithms you could be using to get you to the minimal cost. 
By completing this assignment you will
* understand the intuition between Adam and RMS prop;
* recognize the importance of mini-batch gradient descent;
* learn the effects of momentum on the overall performance of your model.

## 2_Regularization_Methods

It is very important that you regularize your model properly because it could dramatically improve your results.
By completing this assignment you will
* understand that different regularization methods that could help your model;
* implement dropout and see it work on data;
* recognize that a model without regularization gives you a better accuracy on the training set but nor necessarily on the test set;
* understand that you could use both dropout and regularization on your model.

## 3_Gradient_Checking

You will be implementing gradient checking to make sure that your backpropagation implementation is correct. By completing this assignment you will
* implement gradient checking from scratch;
* understand how to use the difference formula to check your backpropagation implementation;
* recognize that your backpropagation algorithm should give you similar results as the ones you got by computing the difference formula;
* learn how to identify which parameter's gradient was computed incorrectly.


