# Deep Learning Mechanics

## 1_L-layer Neural Network

In this assignment you will implement all the building blocks of a 
neural network and use these building blocks to build a neural network 
of any architecture.

## 2_Initialization_Methods

Different initializations lead to different results. 
By completing this assignment you will learn how to use different initializations to
* Speed up the convergence of gradient descent
* Increase the odds of gradient descent converging to a lower training (and generalization) error
* Break symmetry and make sure different hidden units can learn different things

## 2_Optimization_Methods	

There are many different optimization algorithms you could be using to get you to the minimal cost. 
By completing this assignment you will:

* Understand the intuition between Adam and RMS prop
* Recognize the importance of mini-batch gradient descent
* Learn the effects of momentum on the overall performance of your model

## 2_Regularization_Methods

It is very important that you regularize your model properly because it could dramatically improve your results.
By completing this assignment you will:

* Understand that different regularization methods that could help your model.
* Implement dropout and see it work on data.
* Recognize that a model without regularization gives you a better accuracy on the training set but nor necessarily on the test set.
* Understand that you could use both dropout and regularization on your model.

## 3_Gradient_Checking

You will be implementing gradient checking to make sure that your backpropagation implementation is correct. By completing this assignment you will:
* Implement gradient checking from scratch.
* Understand how to use the difference formula to check your backpropagation implementation.
* Recognize that your backpropagation algorithm should give you similar results as the ones you got by computing the difference formula.
* Learn how to identify which parameter's gradient was computed incorrectly.


